{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd0bda1",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0b868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b0e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b30674",
   "metadata": {},
   "source": [
    "#### Download AnnoMI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e4a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## 2. Download AnnoMI Dataset\n",
    "\n",
    "\n",
    "# %%\n",
    "def download_annomi_dataset():\n",
    "    \"\"\"Download and extract AnnoMI dataset\"\"\"\n",
    "    url = \"https://github.com/uccollab/AnnoMI/archive/refs/heads/main.zip\"\n",
    "    zip_path = \"annomi.zip\"\n",
    "    extract_path = \"./data\"\n",
    "   \n",
    "    # Download\n",
    "    if not os.path.exists(extract_path):\n",
    "        print(\"Downloading AnnoMI dataset...\")\n",
    "        response = requests.get(url)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "       \n",
    "        # Extract\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        os.remove(zip_path)\n",
    "        print(\"Dataset downloaded and extracted!\")\n",
    "    else:\n",
    "        print(\"Dataset already exists!\")\n",
    "   \n",
    "    return extract_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795a26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists!\n"
     ]
    }
   ],
   "source": [
    "data_path = download_annomi_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa036f",
   "metadata": {},
   "source": [
    "#### Step 1: Read in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296c0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "## ## 3. Load and Preprocess Data\n",
    "\n",
    "# %%\n",
    "def load_annomi_data(data_path):\n",
    "    \"\"\"Load and preprocess AnnoMI conversations\"\"\"\n",
    "    # The AnnoMI dataset has a simple CSV file\n",
    "    annomi_csv_path = os.path.join(data_path, \"AnnoMI-main\", \"AnnoMI-simple.csv\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(annomi_csv_path):\n",
    "        # Try alternative path\n",
    "        annomi_csv_path = os.path.join(data_path, \"AnnoMI-main\", \"data\", \"AnnoMI-simple.csv\")\n",
    "        \n",
    "    if not os.path.exists(annomi_csv_path):\n",
    "        print(f\"Error: Could not find AnnoMI-simple.csv at {annomi_csv_path}\")\n",
    "        print(f\"Available files in {data_path}:\")\n",
    "        for root, dirs, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                print(os.path.join(root, file))\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Loading data from: {annomi_csv_path}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(annomi_csv_path)\n",
    "    \n",
    "    # Display column names to understand structure\n",
    "    #print(f\"\\nColumns in dataset: {df.columns.tolist()}\")\n",
    "    #print(f\"Dataset shape: {df.shape}\")\n",
    "    #print(f\"\\nFirst few rows:\")\n",
    "    #print(df.head())\n",
    "    \n",
    "    # Typical AnnoMI structure has columns like:\n",
    "    # - utterance_id, session_id, interlocutor (speaker role)\n",
    "    # - utterance_text, main_therapist_behaviour_code, etc.\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    column_mapping = {\n",
    "        'interlocutor': 'role',\n",
    "        'utterance_text': 'text',\n",
    "        'main_therapist_behaviour_code': 'code'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping for columns that exist\n",
    "    df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})\n",
    "    \n",
    "    # Filter out rows without behavior codes (clients don't have codes)\n",
    "    if 'code' in df.columns:\n",
    "        df = df.dropna(subset=['code'])\n",
    "    \n",
    "    # Keep only therapist utterances for forecasting\n",
    "    if 'role' in df.columns:\n",
    "        print(f\"\\nRole distribution:\")\n",
    "        print(df['role'].value_counts())\n",
    "    \n",
    "    # Clean and standardize\n",
    "    if 'text' in df.columns:\n",
    "        df['text'] = df['text'].fillna('').astype(str).str.strip()\n",
    "    \n",
    "    if 'code' in df.columns:\n",
    "        df['code'] = df['code'].fillna('').astype(str).str.strip()\n",
    "        # Remove empty codes\n",
    "        df = df[df['code'] != '']\n",
    "    \n",
    "    #print(f\"\\nAfter preprocessing: {len(df)} utterances\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb990b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ./data\\AnnoMI-main\\AnnoMI-simple.csv\n",
      "\n",
      "Role distribution:\n",
      "role\n",
      "therapist    4882\n",
      "client       4817\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = load_annomi_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc9f74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>mi_quality</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_url</th>\n",
       "      <th>topic</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>role</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>main_therapist_behaviour</th>\n",
       "      <th>client_talk_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>0</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:13</td>\n",
       "      <td>Thanks for filling it out. We give this form t...</td>\n",
       "      <td>question</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>1</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>Sure.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>2</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>So, let's see. It looks that you put-- You dri...</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>3</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>NEW VIDEO: Brief intervention: \"Barbara\"</td>\n",
       "      <td>https://www.youtube.com/watch?v=PaSKcfTmFEk</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>4</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>-and you usually have three to four drinks whe...</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transcript_id mi_quality                               video_title  \\\n",
       "0              0       high  NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "1              0       high  NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "2              0       high  NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "3              0       high  NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "4              0       high  NEW VIDEO: Brief intervention: \"Barbara\"   \n",
       "\n",
       "                                     video_url                         topic  \\\n",
       "0  https://www.youtube.com/watch?v=PaSKcfTmFEk  reducing alcohol consumption   \n",
       "1  https://www.youtube.com/watch?v=PaSKcfTmFEk  reducing alcohol consumption   \n",
       "2  https://www.youtube.com/watch?v=PaSKcfTmFEk  reducing alcohol consumption   \n",
       "3  https://www.youtube.com/watch?v=PaSKcfTmFEk  reducing alcohol consumption   \n",
       "4  https://www.youtube.com/watch?v=PaSKcfTmFEk  reducing alcohol consumption   \n",
       "\n",
       "   utterance_id       role timestamp  \\\n",
       "0             0  therapist  00:00:13   \n",
       "1             1     client  00:00:24   \n",
       "2             2  therapist  00:00:25   \n",
       "3             3     client  00:00:34   \n",
       "4             4  therapist  00:00:34   \n",
       "\n",
       "                                                text main_therapist_behaviour  \\\n",
       "0  Thanks for filling it out. We give this form t...                 question   \n",
       "1                                              Sure.                      NaN   \n",
       "2  So, let's see. It looks that you put-- You dri...          therapist_input   \n",
       "3                                            Mm-hmm.                      NaN   \n",
       "4  -and you usually have three to four drinks whe...          therapist_input   \n",
       "\n",
       "  client_talk_type  \n",
       "0              NaN  \n",
       "1          neutral  \n",
       "2              NaN  \n",
       "3          neutral  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e4bf4",
   "metadata": {},
   "source": [
    "#### Step 2: Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3a6723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total therapist utterances with behavior codes: 4882\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "#### Prepare Data for BERT Classification\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Filter to therapist utterances only\n",
    "therapist_df = df[df['role'] == 'therapist'].copy()\n",
    "\n",
    "# Remove rows with missing behavior codes\n",
    "therapist_df = therapist_df.dropna(subset=['main_therapist_behaviour'])\n",
    "\n",
    "print(f\"Total therapist utterances with behavior codes: {len(therapist_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53c2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map behavior codes to focus on main categories\n",
    "# Adjust this mapping based on your actual codes\n",
    "def map_behavior_codes(code):\n",
    "    \"\"\"Map behavior codes to main categories\"\"\"\n",
    "    code = str(code).upper()\n",
    "    \n",
    "    if 'REFLECTION' in code or code.startswith('R'):\n",
    "        return 'Reflection'\n",
    "    elif 'QUESTION' in code or code.startswith('Q'):\n",
    "        return 'Question'\n",
    "    elif 'THERAPIST_INPUT' in code or code.startswith('T'):\n",
    "        return 'Input'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b87396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4882 entries, 0 to 9698\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   transcript_id             4882 non-null   int64 \n",
      " 1   mi_quality                4882 non-null   object\n",
      " 2   video_title               4882 non-null   object\n",
      " 3   video_url                 4882 non-null   object\n",
      " 4   topic                     4882 non-null   object\n",
      " 5   utterance_id              4882 non-null   int64 \n",
      " 6   role                      4882 non-null   object\n",
      " 7   timestamp                 4882 non-null   object\n",
      " 8   text                      4882 non-null   object\n",
      " 9   main_therapist_behaviour  4882 non-null   object\n",
      " 10  client_talk_type          0 non-null      object\n",
      " 11  behavior_category         4882 non-null   object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 495.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Apply mapping (adjust based on actual column values)\n",
    "therapist_df['behavior_category'] = therapist_df['main_therapist_behaviour'].apply(map_behavior_codes)\n",
    "therapist_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783ee044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4-Class Distribution:\n",
      "behavior_category\n",
      "Other         1586\n",
      "Question      1386\n",
      "Reflection    1296\n",
      "Input          614\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentages:\n",
      "behavior_category\n",
      "Other         32.486686\n",
      "Question      28.390004\n",
      "Reflection    26.546497\n",
      "Input         12.576813\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4-Class Distribution:\")\n",
    "print(therapist_df['behavior_category'].value_counts())\n",
    "print(\"\\nPercentages:\")\n",
    "print(therapist_df['behavior_category'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee65308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = therapist_df['text'].values\n",
    "y = therapist_df['behavior_category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84cb7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Mapping:\n",
      "  Input: 0\n",
      "  Other: 1\n",
      "  Question: 2\n",
      "  Reflection: 3\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(f\"\\nLabel Mapping:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ec3b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 3905\n",
      "Test size: 977\n",
      "\n",
      "Train label distribution:\n",
      "  Input: 491\n",
      "  Other: 1268\n",
      "  Question: 1109\n",
      "  Reflection: 1037\n",
      "\n",
      "Test label distribution:\n",
      "  Input: 123\n",
      "  Other: 318\n",
      "  Question: 277\n",
      "  Reflection: 259\n"
     ]
    }
   ],
   "source": [
    "# Train/Test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nTrain label distribution:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "for idx, count in train_dist.items():\n",
    "    print(f\"  {label_encoder.classes_[idx]}: {count}\")\n",
    "\n",
    "print(\"\\nTest label distribution:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "for idx, count in test_dist.items():\n",
    "    print(f\"  {label_encoder.classes_[idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490e00c",
   "metadata": {},
   "source": [
    "#### Step 3: Create HuggingFace Datasets and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6918019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "#### Tokenize Data for BERT\n",
    "\n",
    "# %%\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_data = Dataset.from_dict({\n",
    "    'text': X_train,\n",
    "    'label': y_train\n",
    "})\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    'text': X_test,\n",
    "    'label': y_test\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7c3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3905/3905 [00:00<00:00, 17598.42 examples/s]\n",
      "Map: 100%|██████████| 977/977 [00:00<00:00, 15130.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Train dataset: 3905 examples\n",
      "Test dataset: 977 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Adjust based on text length analysis\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "test_dataset = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3712c39",
   "metadata": {},
   "source": [
    "#### Step 4: Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58cdfed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "#### Train BERT Classification Model\n",
    "\n",
    "# %%\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Define metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    # Per-class F1 scores\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "    }\n",
    "    \n",
    "    # Add per-class F1 scores\n",
    "    for idx, class_name in enumerate(label_encoder.classes_):\n",
    "        metrics[f'f1_{class_name}'] = f1_per_class[idx]\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a0eeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Scripts\\python.exe\n",
      "transformers: 4.57.1 c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\transformers\\__init__.py\n",
      "accelerate: 1.11.0 c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\accelerate\\__init__.py\n",
      "datasets: 4.3.0 c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\datasets\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "import sys, transformers, accelerate, datasets\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"transformers:\", transformers.__version__, transformers.__file__)\n",
    "print(\"accelerate:\", accelerate.__version__, accelerate.__file__)\n",
    "print(\"datasets:\", datasets.__version__, datasets.__file__)\n",
    "\n",
    "from transformers.utils.versions import require_version\n",
    "require_version(\"accelerate>=0.26.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "833cec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING BERT MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [980/980 1:29:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>F1 Input</th>\n",
       "      <th>F1 Other</th>\n",
       "      <th>F1 Question</th>\n",
       "      <th>F1 Reflection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.510100</td>\n",
       "      <td>0.782481</td>\n",
       "      <td>0.817664</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.775895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>0.502822</td>\n",
       "      <td>0.797099</td>\n",
       "      <td>0.827459</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.906149</td>\n",
       "      <td>0.867725</td>\n",
       "      <td>0.778865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.525517</td>\n",
       "      <td>0.793981</td>\n",
       "      <td>0.823168</td>\n",
       "      <td>0.639405</td>\n",
       "      <td>0.898502</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.770370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.547037</td>\n",
       "      <td>0.799663</td>\n",
       "      <td>0.828359</td>\n",
       "      <td>0.648438</td>\n",
       "      <td>0.904685</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.775194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\lnbco\\OneDrive\\Documents\\GitHub\\interspeech2022-motivational-interviewing\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=980, training_loss=0.4374250402255934, metrics={'train_runtime': 5352.5453, 'train_samples_per_second': 2.918, 'train_steps_per_second': 0.183, 'total_flos': 1027467121274880.0, 'train_loss': 0.4374250402255934, 'epoch': 4.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',     # <-- use eval_strategy here\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    greater_is_better=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"TRAINING BERT MODEL\")\n",
    "trainer.train()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee6eb0",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08925f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "#### Evaluate Model Performance\n",
    "\n",
    "# %%\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n1. Overall Metrics:\")\n",
    "print(f\"   F1-Macro: {test_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"   F1-Weighted: {test_results['eval_f1_weighted']:.4f}\")\n",
    "print(f\"   Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "print(\"\\n2. Per-Class F1 Scores:\")\n",
    "for class_name in label_encoder.classes_:\n",
    "    f1_key = f'eval_f1_{class_name}'\n",
    "    if f1_key in test_results:\n",
    "        print(f\"   {class_name}: {test_results[f1_key]:.4f}\")\n",
    "\n",
    "# Get detailed predictions\n",
    "predictions_output = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions_output.predictions, axis=-1)\n",
    "y_true = predictions_output.label_ids\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n3. Detailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_true, \n",
    "    y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n4. Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=label_encoder.classes_,\n",
    "    columns=label_encoder.classes_\n",
    ")\n",
    "print(cm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
